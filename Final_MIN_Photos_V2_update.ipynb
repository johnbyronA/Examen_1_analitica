{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johnbyronA/Examen_1_analitica/blob/main/Final_MIN_Photos_V2_update.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Vision**\n",
        "\n",
        "GPT-4 with Vision, sometimes referred to as GPT-4V or gpt-4-vision-preview in the API, allows the model to take in images and answer questions about them. Historically, language model systems have been limited by taking in a single input modality, text. For many use cases, this constrained the areas where models like GPT-4 could be used.\n",
        "\n",
        "GPT-4 with vision is currently available to all developers who have access to GPT-4 via the gpt-4-vision-preview model and the Chat Completions API which has been updated to support image inputs. Note that the Assistants **API does not currently support image input**"
      ],
      "metadata": {
        "id": "R2a3tHYktY4K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image models loading: https://platform.openai.com/docs/guides/vision\n",
        "\n",
        "How to input model: https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models"
      ],
      "metadata": {
        "id": "pk71GbwHerqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "bV_KcOJIflVD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d068a4b4-3799-4004-c650-27ca7eb981bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6gH6LVjdwtQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22f73ef9-3b76-44f0-bba4-fa1b19b3df0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.5.0-py3-none-any.whl (223 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/223.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/223.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.7/223.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 openai-1.5.0\n"
          ]
        }
      ],
      "source": [
        "# if needed, install and/or upgrade to the latest version of the OpenAI Python library\n",
        "%pip install --upgrade openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/A_K.txt', 'r') as file:\n",
        "    api_keys = file.read().strip()"
      ],
      "metadata": {
        "id": "Kp-gJs48Aum-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**VISION**"
      ],
      "metadata": {
        "id": "Y4BGXpk2-rlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the OpenAI Python library for calling the OpenAI API\n",
        "# Call direct to GPT-4 but not with parameters\n",
        "# 0.05 USD of costs\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=api_keys)\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-4-vision-preview\",\n",
        "  messages=[\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": [\n",
        "        {\"type\": \"text\", \"text\": \"Please describe the picture completely as we will use the results later to be analyzed with an assistant, we need to focus on body and face position, face expressions, mouth closure, eye contact, and use of glasses and earrings, and finally evaluate the quality of the photo (for graininess and blurryness, please indicate if the photo is recognizable),\"},\n",
        "        {\n",
        "          \"type\": \"image_url\",\n",
        "          \"image_url\": {\n",
        "            \"url\": \"https://raw.githubusercontent.com/johnbyronA/MIN_Check/main/MIN/806358-M.jpeg\",\n",
        "            \"detail\": \"low\" #low will disable the “high res” model. The model will receive a low-res 512px x 512px version of the image, and represent the image with a budget of 65 tokens. This allows the API to return faster responses and consume fewer input tokens for use cases that do not require high detail.\n",
        "          },\n",
        "        },\n",
        "      ],\n",
        "    }\n",
        "  ],\n",
        "  max_tokens=300,\n",
        ")\n",
        "\n",
        "respuesta=response.choices[0]\n",
        "\n",
        "print(response.choices[0])"
      ],
      "metadata": {
        "id": "FY0sxE_SdziR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4da7067b-a9e6-487e-cffc-06bd8e2f18c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"In the image, there is a bald individual likely identifying as male directly facing the camera. The person's head is positioned straight, aligned with the camera lens, providing a full view of the face. The facial expression appears neutral and composed, with no noticeable smile or frown; the mouth is closed. The eyes are looking directly into the camera, which suggests direct eye contact with the viewer.\\n\\nThe person is wearing a pair of eyeglasses with thick black frames. There are no earrings visible in the photo. As for attire, a glimpse of a collar from a top garment that appears to be a jacket can be seen, and underneath, possibly a dark shirt. The background is a plain, light-colored wall that offers a clean and uncluttered backdrop.\\n\\nEvaluating the quality of the photo, there is good illumination that clearly showcases the individual's features without any apparent harsh shadows or overexposure. The photo exhibits high resolution with discernible details such as the texture of the skin, the individual hairs of the beard, and the fabric of the clothing. There is no graininess or blurriness, and the image is sharp and easily recognizable. Overall, the quality of the photo is very high, making it suitable for detailed analysis.\", role='assistant', function_call=None, tool_calls=None))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ASSISTANT**"
      ],
      "metadata": {
        "id": "8nE4TmbB-zv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an assistant using the OpenAI client\n",
        "assistant = client.beta.assistants.create(\n",
        "    name=\"summarizer\",    # 'name' assigns a name to the assistant. Here, the assistant is named \"Ivisa tutor\"\n",
        "    #instructions=\"The model will always analyze the description of the picture. such as facial features (face position, Smiling (not recommended if teeth are highly visible), eyes direction, earrings size and glasses use (not recommended)), positioning, and overall photo quality. If a photo can meet the criteria for background uniformity (check if it can be easily edited to achieve a compliant result if it doesn´t complies the first time), facial visibility, proper positioning, eye contact, mouth closure, and absence of obstructive accessories (glasses with reflections), it will be classified as NON MIN (No more info needed) otherwise, it will be classified as MIN stating the reasons why. This approach ensures a comprehensive evaluation, focusing on both the current state of the photo and the feasibility of modifications to meet the necessary standards.\",\n",
        "    instructions=\"The model will consistently analyze the picture description, encompassing facial features (including face position and smiling, the latter not recommended if teeth are highly visible), eyes direction (uniform and focused on the camera, with any eye deviation considered a natural condition), earrings size, and glasses use (not recommended). Additionally, it evaluates positioning, overall photo quality (recognizability, even if is a photo of a photo), facial visibility, proper positioning, mouth closure, and absence of obstructive accessories. If the aforementioned criteria are met, the classification is NON MIN (No more info needed); otherwise, it is classified as MIN, with reasons provided (e.g., having glasses is a crucial criterion for a MIN result). Background considerations are expressly excluded. This approach ensures a thorough evaluation, addressing both the current state of the photo and the feasibility of modifications to meet necessary standards.\",\n",
        "\n",
        "    tools=[{\"type\": \"code_interpreter\"}],\n",
        "    model=\"gpt-4-1106-preview\"\n",
        ")"
      ],
      "metadata": {
        "id": "X0aB0Y6O0UIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(assistant)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Hc5aneTzJaw",
        "outputId": "57b99c4a-b5af-46f3-d5d1-9552bc847247"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assistant(id='asst_5ZP8C8c4VhXkNDNnG3TgEj2T', created_at=1702921193, description=None, file_ids=[], instructions='The model will consistently analyze the picture description, encompassing facial features (including face position and smiling, the latter not recommended if teeth are highly visible), eyes direction (uniform and focused on the camera, with any eye deviation considered a natural condition), earrings size, and glasses use (not recommended). Additionally, it evaluates positioning, overall photo quality (recognizability, even if is a photo of a photo), facial visibility, proper positioning, mouth closure, and absence of obstructive accessories. If the aforementioned criteria are met, the classification is NON MIN (No more info needed); otherwise, it is classified as MIN, with reasons provided (e.g., having glasses is a crucial criterion for a MIN result). Background considerations are expressly excluded. This approach ensures a thorough evaluation, addressing both the current state of the photo and the feasibility of modifications to meet necessary standards.', metadata={}, model='gpt-4-1106-preview', name='summarizer', object='assistant', tools=[ToolCodeInterpreter(type='code_interpreter')])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppose that 'respuesta' contains the provided format\n",
        "respuestadef = str(respuesta)\n",
        "\n",
        "# \"Use a regular expression to find the content of the 'content' parameter.\"\n",
        "import re\n",
        "\n",
        "contenido_match = re.search(r'content=\"([^\"]+)\"', respuestadef)\n",
        "if contenido_match:\n",
        "    contenido = contenido_match.group(1)\n",
        "    print(contenido)\n",
        "else:\n",
        "    print(\"No se encontró el parámetro 'content' en la respuesta.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obc_GT557hYP",
        "outputId": "ebbd9c91-4e7b-40b3-8f4a-204de3fafc6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the image, there is a bald individual likely identifying as male directly facing the camera. The person's head is positioned straight, aligned with the camera lens, providing a full view of the face. The facial expression appears neutral and composed, with no noticeable smile or frown; the mouth is closed. The eyes are looking directly into the camera, which suggests direct eye contact with the viewer.\\n\\nThe person is wearing a pair of eyeglasses with thick black frames. There are no earrings visible in the photo. As for attire, a glimpse of a collar from a top garment that appears to be a jacket can be seen, and underneath, possibly a dark shirt. The background is a plain, light-colored wall that offers a clean and uncluttered backdrop.\\n\\nEvaluating the quality of the photo, there is good illumination that clearly showcases the individual's features without any apparent harsh shadows or overexposure. The photo exhibits high resolution with discernible details such as the texture of the skin, the individual hairs of the beard, and the fabric of the clothing. There is no graininess or blurriness, and the image is sharp and easily recognizable. Overall, the quality of the photo is very high, making it suitable for detailed analysis.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**THREAD INSTRUCTIONS**"
      ],
      "metadata": {
        "id": "zEHAA0xm-7wF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start a new conversation thread and get the thread_id\n",
        "thread = client.beta.threads.create()\n",
        "\n",
        "# Get the thread_id from the newly created thread\n",
        "thread_id = thread.id\n",
        "\n",
        "# Instructions new variable\n",
        "#Instruction_two=\"The model will always analyze the description of the picture. such as facial features (face position, Smiling (not recommended if teeth are highly visible), eyes direction, earrings size and glasses use (not recommended)), positioning, and overall photo quality. If a photo can meet the criteria for background uniformity (check if it can be easily edited to achieve a compliant result, if it does, it will help to be a NO MIN), facial visibility, proper positioning, eye contact, mouth closure, and absence of obstructive accessories, it will be classified as NON MIN (No more info needed) otherwise, it will be classified as MIN stating the reasons why (Having glasses is a very important criteria for a MIN result). This approach ensures a comprehensive evaluation, focusing on both the current state of the photo and the feasibility of modifications to meet the necessary standards.\"\n",
        "Instruction_two=\"The model will consistently analyze the picture description, encompassing facial features (including face position and smiling, the latter not recommended if teeth are highly visible), eyes direction (uniform and focused on the camera, with any eye deviation considered a natural condition), earrings size, and glasses use (If you can see visible glasses contour around the eyes, It will be an important MIN reason). Additionally, it evaluates positioning, overall photo quality (recognizability, even if is a photo of a photo), facial visibility, proper positioning, mouth closure, and absence of obstructive accessories. If the aforementioned criteria are met, the classification is NON MIN (No more info needed); otherwise, it is classified as MIN, with reasons provided (e.g., having glasses is a crucial criterion for a MIN result). Background considerations are expressly excluded. This approach ensures a thorough evaluation, addressing both the current state of the photo and the feasibility of modifications to meet necessary standards.\"\n",
        "\n",
        "\n",
        "# Now you can use this thread_id to send messages in this thread\n",
        "message = client.beta.threads.messages.create(\n",
        "    thread_id=thread_id,\n",
        "    role=\"user\",\n",
        "  #content=f\"please interpretate this and give me feedback on a maximun of 200 tokens: {str(contenido)}\"\n",
        "  content = f\"Based on the following image description provided by the Vision API: '{str(contenido)}', please evaluate based on the next instruction: {Instruction_two}\"\n",
        ")\n",
        "\n",
        "print(message.content)"
      ],
      "metadata": {
        "id": "4JSTjdN30xqC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6584ec87-4e2c-44b8-a39e-b0317cc727c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MessageContentText(text=Text(annotations=[], value=\"Based on the following image description provided by the Vision API: 'In the image, there is a bald individual likely identifying as male directly facing the camera. The person's head is positioned straight, aligned with the camera lens, providing a full view of the face. The facial expression appears neutral and composed, with no noticeable smile or frown; the mouth is closed. The eyes are looking directly into the camera, which suggests direct eye contact with the viewer.\\\\n\\\\nThe person is wearing a pair of eyeglasses with thick black frames. There are no earrings visible in the photo. As for attire, a glimpse of a collar from a top garment that appears to be a jacket can be seen, and underneath, possibly a dark shirt. The background is a plain, light-colored wall that offers a clean and uncluttered backdrop.\\\\n\\\\nEvaluating the quality of the photo, there is good illumination that clearly showcases the individual's features without any apparent harsh shadows or overexposure. The photo exhibits high resolution with discernible details such as the texture of the skin, the individual hairs of the beard, and the fabric of the clothing. There is no graininess or blurriness, and the image is sharp and easily recognizable. Overall, the quality of the photo is very high, making it suitable for detailed analysis.', please evaluate based on the next instruction: The model will consistently analyze the picture description, encompassing facial features (including face position and smiling, the latter not recommended if teeth are highly visible), eyes direction (uniform and focused on the camera, with any eye deviation considered a natural condition), earrings size, and glasses use (If you can see visible glasses contour around the eyes, It will be an important MIN reason). Additionally, it evaluates positioning, overall photo quality (recognizability, even if is a photo of a photo), facial visibility, proper positioning, mouth closure, and absence of obstructive accessories. If the aforementioned criteria are met, the classification is NON MIN (No more info needed); otherwise, it is classified as MIN, with reasons provided (e.g., having glasses is a crucial criterion for a MIN result). Background considerations are expressly excluded. This approach ensures a thorough evaluation, addressing both the current state of the photo and the feasibility of modifications to meet necessary standards.\"), type='text')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RUN**"
      ],
      "metadata": {
        "id": "JlDDn8Ec_DDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a run (a single execution or interaction) within a conversation thread using the OpenAI client\n",
        "run = client.beta.threads.runs.create(\n",
        "  thread_id=thread.id,      # 'thread_id' specifies the ID of the thread in which this run will occur.\n",
        "  assistant_id=assistant.id,      # 'assistant_id' specifies the ID of the assistant to be used for this run.\n",
        "\n",
        "  # 'instructions' provide specific guidelines or tasks for the assistant in this run.\n",
        "  instructions=f\"please categorize the answer as MIN(more info needed) or no MIN according to the content parameter, additionally, put the reasons according to the analyzed description. Divide them as GOV reasons for the summary of the information provided, and also divide them in Ivisa Reasons for those who have chances of editing to make the photo compliant with the requirements\"\n",
        ")\n",
        "\n",
        "print(run)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCpCZkvx-VPE",
        "outputId": "8c9946fe-0a23-49a4-9f4b-4818507e5059"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run(id='run_mMr6ioPimF4b8Y6OSaV7Cj03', assistant_id='asst_5ZP8C8c4VhXkNDNnG3TgEj2T', cancelled_at=None, completed_at=None, created_at=1702921210, expires_at=1702921810, failed_at=None, file_ids=[], instructions='please categorize the answer as MIN(more info needed) or no MIN according to the content parameter, additionally, put the reasons according to the analyzed description. Divide them as GOV reasons for the summary of the information provided, and also divide them in Ivisa Reasons for those who have chances of editing to make the photo compliant with the requirements', last_error=None, metadata={}, model='gpt-4-1106-preview', object='thread.run', required_action=None, started_at=None, status='queued', thread_id='thread_ptkbJYxaNjDTUF9EKdADIBEX', tools=[ToolAssistantToolsCode(type='code_interpreter')])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT NOTE:** API Response Has an average delay of 30 seconds"
      ],
      "metadata": {
        "id": "M0k4fZ7bzkNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing all messages in a specific conversation thread using the OpenAI client\n",
        "\n",
        "messages = client.beta.threads.messages.list(        # Here, it uses the ID of a previously created or identified thread (thread.id).\n",
        "  thread_id=thread.id\n",
        ")\n",
        "\n",
        "list(messages)  # The 'list()' function is used here to convert the iterable object returned by 'messages.list' into a standard Python list. This makes it easier to work with the messages,\n",
        "\n"
      ],
      "metadata": {
        "id": "QatVna0V-zzu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "651e40b7-ea3d-4763-b46e-5f17190928fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[ThreadMessage(id='msg_Tccq9Shr69mnTvtzgOj8klCs', assistant_id='asst_5ZP8C8c4VhXkNDNnG3TgEj2T', content=[MessageContentText(text=Text(annotations=[], value=\"The image description provided indicates that the photo already meets several of the requirements specified:\\n\\n- **Facial Features and Position**: The individual is facing the camera with the head positioned straight, and the facial expression is neutral with the mouth closed.\\n- **Eyes Direction**: The eyes are looking directly into the camera, suggesting direct eye contact without mention of any deviation.\\n- **Earrings Size**: There are no earrings visible in the photo, so this is not a concern.\\n- **Positioning and Photo Quality**: The individual's features are clearly showcased with good lighting, and the photo is of high resolution without graininess or blurriness. The photo seems to be easily recognizable.\\n- **Facial Visibility**: The full face is visible in the photo, and there is no mention of any obstructive accessories.\\n- **Mouth Closure**: The mouth is closed, which is within the requirements.\\n\\nHowever, there is one aspect that could lead to a MIN classification:\\n\\n- **Glasses Use**: The person is wearing eyeglasses with thick black frames, which can be a crucial reason for MIN since the contour around the eyes can be an important factor.\\n\\nGiven these points, the classification for this image description would be:\\n\\nMIN (More Info Needed)\\n\\n**GOV Reasons**:\\n- Presence of glasses, which could potentially obstruct a clear view of the eyes depending on the reflections and thickness of the frames.\\n\\n**iVisa Reasons**:\\n- If the glasses produce glare that obscures the eyes or if the frames are so thick that they interfere with the visibility of the eye region, an edit may be required to make the photo compliant.\\n- An assessment of whether the glasses can stay on without causing any issues would be necessary. If they are against the requirements, the individual may be asked to retake the photo without glasses.\\n- It is possible that guidelines could allow glasses as long as the eyes are clearly visible and there is no glare, in which case an edit to remove any glare while preserving the eyes might be an option if such a service is available.\\n\\nOther aspects are not mentioned to require further information or adjustments, so no additional iVisa editing reasons are provided.\"), type='text')], created_at=1702921214, file_ids=[], metadata={}, object='thread.message', role='assistant', run_id='run_mMr6ioPimF4b8Y6OSaV7Cj03', thread_id='thread_ptkbJYxaNjDTUF9EKdADIBEX'),\n",
              " ThreadMessage(id='msg_4jE92GZrWdEgkVH7dwSQOHAt', assistant_id=None, content=[MessageContentText(text=Text(annotations=[], value=\"Based on the following image description provided by the Vision API: 'In the image, there is a bald individual likely identifying as male directly facing the camera. The person's head is positioned straight, aligned with the camera lens, providing a full view of the face. The facial expression appears neutral and composed, with no noticeable smile or frown; the mouth is closed. The eyes are looking directly into the camera, which suggests direct eye contact with the viewer.\\\\n\\\\nThe person is wearing a pair of eyeglasses with thick black frames. There are no earrings visible in the photo. As for attire, a glimpse of a collar from a top garment that appears to be a jacket can be seen, and underneath, possibly a dark shirt. The background is a plain, light-colored wall that offers a clean and uncluttered backdrop.\\\\n\\\\nEvaluating the quality of the photo, there is good illumination that clearly showcases the individual's features without any apparent harsh shadows or overexposure. The photo exhibits high resolution with discernible details such as the texture of the skin, the individual hairs of the beard, and the fabric of the clothing. There is no graininess or blurriness, and the image is sharp and easily recognizable. Overall, the quality of the photo is very high, making it suitable for detailed analysis.', please evaluate based on the next instruction: The model will consistently analyze the picture description, encompassing facial features (including face position and smiling, the latter not recommended if teeth are highly visible), eyes direction (uniform and focused on the camera, with any eye deviation considered a natural condition), earrings size, and glasses use (If you can see visible glasses contour around the eyes, It will be an important MIN reason). Additionally, it evaluates positioning, overall photo quality (recognizability, even if is a photo of a photo), facial visibility, proper positioning, mouth closure, and absence of obstructive accessories. If the aforementioned criteria are met, the classification is NON MIN (No more info needed); otherwise, it is classified as MIN, with reasons provided (e.g., having glasses is a crucial criterion for a MIN result). Background considerations are expressly excluded. This approach ensures a thorough evaluation, addressing both the current state of the photo and the feasibility of modifications to meet necessary standards.\"), type='text')], created_at=1702921206, file_ids=[], metadata={}, object='thread.message', role='user', run_id=None, thread_id='thread_ptkbJYxaNjDTUF9EKdADIBEX')]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Initialize a variable to store the first 'value' found\n",
        "primer_valor = None\n",
        "\n",
        "# Iterate through each message in the list\n",
        "for message in messages:\n",
        "    # Convert the message to a string\n",
        "    message_str = str(message)\n",
        "\n",
        "    # Use a regular expression to find the first content of the 'value' parameter\n",
        "    valor_match = re.search(r'value=\"([^\"]+)\"', message_str)\n",
        "\n",
        "    # If a value is found, store it in 'first_value' and exit the loop\n",
        "    if valor_match:\n",
        "        primer_valor = valor_match.group(1)\n",
        "        break  # Exit the loop once the first value is found\n",
        "\n",
        "# Print the first 'value' found\n",
        "if primer_valor:\n",
        "    print(primer_valor)\n",
        "else:\n",
        "    print(\"The 'value' parameter was not found in the messages.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BsSq3BmDwmY",
        "outputId": "6f1c2d9a-a393-4983-f405-82f252574fc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The image description provided indicates that the photo already meets several of the requirements specified:\\n\\n- **Facial Features and Position**: The individual is facing the camera with the head positioned straight, and the facial expression is neutral with the mouth closed.\\n- **Eyes Direction**: The eyes are looking directly into the camera, suggesting direct eye contact without mention of any deviation.\\n- **Earrings Size**: There are no earrings visible in the photo, so this is not a concern.\\n- **Positioning and Photo Quality**: The individual's features are clearly showcased with good lighting, and the photo is of high resolution without graininess or blurriness. The photo seems to be easily recognizable.\\n- **Facial Visibility**: The full face is visible in the photo, and there is no mention of any obstructive accessories.\\n- **Mouth Closure**: The mouth is closed, which is within the requirements.\\n\\nHowever, there is one aspect that could lead to a MIN classification:\\n\\n- **Glasses Use**: The person is wearing eyeglasses with thick black frames, which can be a crucial reason for MIN since the contour around the eyes can be an important factor.\\n\\nGiven these points, the classification for this image description would be:\\n\\nMIN (More Info Needed)\\n\\n**GOV Reasons**:\\n- Presence of glasses, which could potentially obstruct a clear view of the eyes depending on the reflections and thickness of the frames.\\n\\n**iVisa Reasons**:\\n- If the glasses produce glare that obscures the eyes or if the frames are so thick that they interfere with the visibility of the eye region, an edit may be required to make the photo compliant.\\n- An assessment of whether the glasses can stay on without causing any issues would be necessary. If they are against the requirements, the individual may be asked to retake the photo without glasses.\\n- It is possible that guidelines could allow glasses as long as the eyes are clearly visible and there is no glare, in which case an edit to remove any glare while preserving the eyes might be an option if such a service is available.\\n\\nOther aspects are not mentioned to require further information or adjustments, so no additional iVisa editing reasons are provided.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a run (a single execution or interaction) within a conversation thread using the OpenAI client\n",
        "run = client.beta.threads.runs.create(\n",
        "  thread_id=thread.id,      # 'thread_id' specifies the ID of the thread in which this run will occur.\n",
        "  assistant_id=assistant.id,      # 'assistant_id' specifies the ID of the assistant to be used for this run.\n",
        "\n",
        "  # 'instructions' provide specific guidelines or tasks for the assistant in this run.\n",
        "  instructions=\"please categorize the answer as MIN(more info needed) or no MIN according to the content parameter, additionally, put the reasons according to the analyzed description. Divide them as GOV reasons for the summary of the information provided, and also divide them in Ivisa Reasons for those who have chances of editing to make the photo compliant with the requirements\",\n",
        ")\n",
        "\n",
        "print(run)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEVvPxw0-eNf",
        "outputId": "e9bbaaf3-70a7-4f8b-9c18-9f6b7c510410"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run(id='run_bTMReVOiTnwncYKxs1QAOsDd', assistant_id='asst_5ZP8C8c4VhXkNDNnG3TgEj2T', cancelled_at=None, completed_at=None, created_at=1702921327, expires_at=1702921927, failed_at=None, file_ids=[], instructions='please categorize the answer as MIN(more info needed) or no MIN according to the content parameter, additionally, put the reasons according to the analyzed description. Divide them as GOV reasons for the summary of the information provided, and also divide them in Ivisa Reasons for those who have chances of editing to make the photo compliant with the requirements', last_error=None, metadata={}, model='gpt-4-1106-preview', object='thread.run', required_action=None, started_at=None, status='queued', thread_id='thread_ptkbJYxaNjDTUF9EKdADIBEX', tools=[ToolAssistantToolsCode(type='code_interpreter')])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing all messages in a specific conversation thread using the OpenAI client\n",
        "\n",
        "messages = client.beta.threads.messages.list(        # Here, it uses the ID of a previously created or identified thread (thread.id).\n",
        "  thread_id=thread.id\n",
        ")\n",
        "\n",
        "list(messages)  # The 'list()' function is used here to convert the iterable object returned by 'messages.list' into a standard Python list. This makes it easier to work with the messages,\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3wJSBW4_T7t",
        "outputId": "1b1233a0-e746-463c-f005-3f427f21123b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[ThreadMessage(id='msg_Jj5SnOCmAd9RrVo0dXDmRQ4k', assistant_id='asst_5ZP8C8c4VhXkNDNnG3TgEj2T', content=[MessageContentText(text=Text(annotations=[], value=\"MIN\\n\\n**GOV Reasons**:\\n- Presence of glasses can be a crucial reason for MIN classification if the glasses' frames obstruct the visibility of the eyes.\\n\\n**iVisa Reasons**:\\n- The use of glasses might be problematic if they cause a glare or if the frames cover any part of the eyes, which would justify a request for a photo without glasses or potentially editing to reduce glare while maintaining the visibility of the eyes.\"), type='text')], created_at=1702921328, file_ids=[], metadata={}, object='thread.message', role='assistant', run_id='run_bTMReVOiTnwncYKxs1QAOsDd', thread_id='thread_ptkbJYxaNjDTUF9EKdADIBEX'),\n",
              " ThreadMessage(id='msg_Tccq9Shr69mnTvtzgOj8klCs', assistant_id='asst_5ZP8C8c4VhXkNDNnG3TgEj2T', content=[MessageContentText(text=Text(annotations=[], value=\"The image description provided indicates that the photo already meets several of the requirements specified:\\n\\n- **Facial Features and Position**: The individual is facing the camera with the head positioned straight, and the facial expression is neutral with the mouth closed.\\n- **Eyes Direction**: The eyes are looking directly into the camera, suggesting direct eye contact without mention of any deviation.\\n- **Earrings Size**: There are no earrings visible in the photo, so this is not a concern.\\n- **Positioning and Photo Quality**: The individual's features are clearly showcased with good lighting, and the photo is of high resolution without graininess or blurriness. The photo seems to be easily recognizable.\\n- **Facial Visibility**: The full face is visible in the photo, and there is no mention of any obstructive accessories.\\n- **Mouth Closure**: The mouth is closed, which is within the requirements.\\n\\nHowever, there is one aspect that could lead to a MIN classification:\\n\\n- **Glasses Use**: The person is wearing eyeglasses with thick black frames, which can be a crucial reason for MIN since the contour around the eyes can be an important factor.\\n\\nGiven these points, the classification for this image description would be:\\n\\nMIN (More Info Needed)\\n\\n**GOV Reasons**:\\n- Presence of glasses, which could potentially obstruct a clear view of the eyes depending on the reflections and thickness of the frames.\\n\\n**iVisa Reasons**:\\n- If the glasses produce glare that obscures the eyes or if the frames are so thick that they interfere with the visibility of the eye region, an edit may be required to make the photo compliant.\\n- An assessment of whether the glasses can stay on without causing any issues would be necessary. If they are against the requirements, the individual may be asked to retake the photo without glasses.\\n- It is possible that guidelines could allow glasses as long as the eyes are clearly visible and there is no glare, in which case an edit to remove any glare while preserving the eyes might be an option if such a service is available.\\n\\nOther aspects are not mentioned to require further information or adjustments, so no additional iVisa editing reasons are provided.\"), type='text')], created_at=1702921214, file_ids=[], metadata={}, object='thread.message', role='assistant', run_id='run_mMr6ioPimF4b8Y6OSaV7Cj03', thread_id='thread_ptkbJYxaNjDTUF9EKdADIBEX'),\n",
              " ThreadMessage(id='msg_4jE92GZrWdEgkVH7dwSQOHAt', assistant_id=None, content=[MessageContentText(text=Text(annotations=[], value=\"Based on the following image description provided by the Vision API: 'In the image, there is a bald individual likely identifying as male directly facing the camera. The person's head is positioned straight, aligned with the camera lens, providing a full view of the face. The facial expression appears neutral and composed, with no noticeable smile or frown; the mouth is closed. The eyes are looking directly into the camera, which suggests direct eye contact with the viewer.\\\\n\\\\nThe person is wearing a pair of eyeglasses with thick black frames. There are no earrings visible in the photo. As for attire, a glimpse of a collar from a top garment that appears to be a jacket can be seen, and underneath, possibly a dark shirt. The background is a plain, light-colored wall that offers a clean and uncluttered backdrop.\\\\n\\\\nEvaluating the quality of the photo, there is good illumination that clearly showcases the individual's features without any apparent harsh shadows or overexposure. The photo exhibits high resolution with discernible details such as the texture of the skin, the individual hairs of the beard, and the fabric of the clothing. There is no graininess or blurriness, and the image is sharp and easily recognizable. Overall, the quality of the photo is very high, making it suitable for detailed analysis.', please evaluate based on the next instruction: The model will consistently analyze the picture description, encompassing facial features (including face position and smiling, the latter not recommended if teeth are highly visible), eyes direction (uniform and focused on the camera, with any eye deviation considered a natural condition), earrings size, and glasses use (If you can see visible glasses contour around the eyes, It will be an important MIN reason). Additionally, it evaluates positioning, overall photo quality (recognizability, even if is a photo of a photo), facial visibility, proper positioning, mouth closure, and absence of obstructive accessories. If the aforementioned criteria are met, the classification is NON MIN (No more info needed); otherwise, it is classified as MIN, with reasons provided (e.g., having glasses is a crucial criterion for a MIN result). Background considerations are expressly excluded. This approach ensures a thorough evaluation, addressing both the current state of the photo and the feasibility of modifications to meet necessary standards.\"), type='text')], created_at=1702921206, file_ids=[], metadata={}, object='thread.message', role='user', run_id=None, thread_id='thread_ptkbJYxaNjDTUF9EKdADIBEX')]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a variable to store the first 'value' found\n",
        "resumen_valor = None\n",
        "\n",
        "# Iterate through each message in the list\n",
        "for message in messages:\n",
        "    # Convert the message to a string\n",
        "    message_str = str(message)\n",
        "\n",
        "    # Use a regular expression to find the first content of the 'value' parameter\n",
        "    valor_match = re.search(r'value=\"([^\"]+)\"', message_str)\n",
        "\n",
        "    # If a value is found, store it in 'first_value' and exit the loop\n",
        "    if valor_match:\n",
        "        resumen_valor = valor_match.group(1)\n",
        "        break  # Exit the loop once the first value is found\n",
        "\n",
        "# Print the first 'value' found\n",
        "if resumen_valor:\n",
        "    print(resumen_valor)\n",
        "else:\n",
        "    print(\"The 'value' parameter was not found in the messages.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPsX7Q7RBnS1",
        "outputId": "1548d87c-d926-4f59-d728-ff910f3cc2b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MIN\\n\\n**GOV Reasons**:\\n- Presence of glasses can be a crucial reason for MIN classification if the glasses' frames obstruct the visibility of the eyes.\\n\\n**iVisa Reasons**:\\n- The use of glasses might be problematic if they cause a glare or if the frames cover any part of the eyes, which would justify a request for a photo without glasses or potentially editing to reduce glare while maintaining the visibility of the eyes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "print(sys.version.split(' ')[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yd4wWl1I6TYi",
        "outputId": "c2e71e28-6c68-4f5b-ceb6-86d32d689f35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "print(\"Versión de re:\", re.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1hip1su7AH6",
        "outputId": "382c3b6f-0969-49d1-ab8f-346ce193152a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Versión de re: 2.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "print(\"Versión de openai:\", openai.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E063Fpz77EJy",
        "outputId": "8918fb01-4ef4-47c7-b1ec-d32d13c8c0ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Versión de openai: 1.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.colab\n",
        "print(\"Versión de google.colab:\", google.colab.__version__)\n"
      ],
      "metadata": {
        "id": "zUCpAe637OQy",
        "outputId": "073009ff-4c43-4e1a-f451-63d4c2c7b666",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Versión de google.colab: 0.0.1a2\n"
          ]
        }
      ]
    }
  ]
}